{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29c479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MiAplicacion\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9eab980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Player: string (nullable = true)\n",
      " |-- Squad: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- Born: float (nullable = true)\n",
      " |-- MP: float (nullable = true)\n",
      " |-- Starts: float (nullable = true)\n",
      " |-- Min: float (nullable = true)\n",
      " |-- Gls: float (nullable = true)\n",
      " |-- Ast: float (nullable = true)\n",
      " |-- YellowC: float (nullable = true)\n",
      " |-- RedC: float (nullable = true)\n",
      " |-- PrgC: float (nullable = true)\n",
      " |-- PrgP: float (nullable = true)\n",
      " |-- PrgR: float (nullable = true)\n",
      " |-- Fls: float (nullable = true)\n",
      " |-- Off: float (nullable = true)\n",
      " |-- Crosses: float (nullable = true)\n",
      " |-- Recov: float (nullable = true)\n",
      " |-- Aerialwon%: float (nullable = true)\n",
      " |-- Tack_Def_3rd: float (nullable = true)\n",
      " |-- Tack_Mid_3rd: float (nullable = true)\n",
      " |-- Tack_Att_3rd: float (nullable = true)\n",
      " |-- Tkl%: float (nullable = true)\n",
      " |-- Tkl: float (nullable = true)\n",
      " |-- TklW: float (nullable = true)\n",
      " |-- Int: float (nullable = true)\n",
      " |-- Blocks: float (nullable = true)\n",
      " |-- Block_Shots: float (nullable = true)\n",
      " |-- Block_Pass: float (nullable = true)\n",
      " |-- Clearences: float (nullable = true)\n",
      " |-- Err: float (nullable = true)\n",
      " |-- Pass_cmp: float (nullable = true)\n",
      " |-- Pass_cmp%: float (nullable = true)\n",
      " |-- Pass_Short: float (nullable = true)\n",
      " |-- Pass_cmp_Short%: float (nullable = true)\n",
      " |-- Pass_Medium: float (nullable = true)\n",
      " |-- Pass_cmp_Medium%: float (nullable = true)\n",
      " |-- Pass_Long: float (nullable = true)\n",
      " |-- Pass_cmp_Long%: float (nullable = true)\n",
      " |-- xAG: float (nullable = true)\n",
      " |-- xA: float (nullable = true)\n",
      " |-- A-xAG: float (nullable = true)\n",
      " |-- Pass_cmp_Att_3rd: float (nullable = true)\n",
      " |-- PPA: float (nullable = true)\n",
      " |-- CrsPA: float (nullable = true)\n",
      " |-- Touch_Def_3rd: float (nullable = true)\n",
      " |-- Touch_Mid_3rd: float (nullable = true)\n",
      " |-- Touch_Att_3rd: float (nullable = true)\n",
      " |-- Touch_Att_Pen: float (nullable = true)\n",
      " |-- Touch_Live: float (nullable = true)\n",
      " |-- drib_Att: float (nullable = true)\n",
      " |-- drib_Succ%: float (nullable = true)\n",
      " |-- Tckl_Drib: float (nullable = true)\n",
      " |-- Tckl_Drib%: float (nullable = true)\n",
      " |-- PrgDist: float (nullable = true)\n",
      " |-- Carries_Att_3rd: float (nullable = true)\n",
      " |-- Carries_Att_Pen: float (nullable = true)\n",
      " |-- fail_To_Gain_Control: float (nullable = true)\n",
      " |-- Loss_Control_Tackle: float (nullable = true)\n",
      " |-- Sh: float (nullable = true)\n",
      " |-- SoT: float (nullable = true)\n",
      " |-- G/Sh: float (nullable = true)\n",
      " |-- xG: float (nullable = true)\n",
      " |-- npxG: float (nullable = true)\n",
      " |-- npxG/Sh: float (nullable = true)\n",
      " |-- G-xG: float (nullable = true)\n",
      " |-- Pos: string (nullable = true)\n",
      " |-- Height: float (nullable = true)\n",
      " |-- Value: float (nullable = true)\n",
      " |-- GA: float (nullable = true)\n",
      " |-- SoTA: float (nullable = true)\n",
      " |-- Save%: float (nullable = true)\n",
      " |-- CS%: float (nullable = true)\n",
      " |-- PKatt: float (nullable = true)\n",
      " |-- P_Save%: float (nullable = true)\n",
      " |-- PSxG: float (nullable = true)\n",
      " |-- PSxG/SoT: float (nullable = true)\n",
      " |-- PSxG+/-: float (nullable = true)\n",
      " |-- Launch%: float (nullable = true)\n",
      " |-- Crosses_Opp: float (nullable = true)\n",
      " |-- Crosses_stp%: float (nullable = true)\n",
      " |-- #OPA: float (nullable = true)\n",
      " |-- Gls_per90: double (nullable = true)\n",
      " |-- Ast_per90: double (nullable = true)\n",
      " |-- PrgC_per90: double (nullable = true)\n",
      " |-- PrgP_per90: double (nullable = true)\n",
      " |-- PrgR_per90: double (nullable = true)\n",
      " |-- Fls_per90: double (nullable = true)\n",
      " |-- Off_per90: double (nullable = true)\n",
      " |-- Crosses_per90: double (nullable = true)\n",
      " |-- Recov_per90: double (nullable = true)\n",
      " |-- Tack_Def_3rd_per90: double (nullable = true)\n",
      " |-- Tack_Mid_3rd_per90: double (nullable = true)\n",
      " |-- Tack_Att_3rd_per90: double (nullable = true)\n",
      " |-- Tkl_per90: double (nullable = true)\n",
      " |-- TklW_per90: double (nullable = true)\n",
      " |-- Int_per90: double (nullable = true)\n",
      " |-- Blocks_per90: double (nullable = true)\n",
      " |-- Block_Shots_per90: double (nullable = true)\n",
      " |-- Block_Pass_per90: double (nullable = true)\n",
      " |-- Clearences_per90: double (nullable = true)\n",
      " |-- Err_per90: double (nullable = true)\n",
      " |-- Pass_cmp_per90: double (nullable = true)\n",
      " |-- Pass_Short_per90: double (nullable = true)\n",
      " |-- Pass_Medium_per90: double (nullable = true)\n",
      " |-- Pass_Long_per90: double (nullable = true)\n",
      " |-- xAG_per90: double (nullable = true)\n",
      " |-- xA_per90: double (nullable = true)\n",
      " |-- A-xAG_per90: double (nullable = true)\n",
      " |-- Pass_cmp_Att_3rd_per90: double (nullable = true)\n",
      " |-- PPA_per90: double (nullable = true)\n",
      " |-- CrsPA_per90: double (nullable = true)\n",
      " |-- Touch_Def_3rd_per90: double (nullable = true)\n",
      " |-- Touch_Mid_3rd_per90: double (nullable = true)\n",
      " |-- Touch_Att_3rd_per90: double (nullable = true)\n",
      " |-- Touch_Att_Pen_per90: double (nullable = true)\n",
      " |-- Touch_Live_per90: double (nullable = true)\n",
      " |-- drib_Att_per90: double (nullable = true)\n",
      " |-- Tckl_Drib_per90: double (nullable = true)\n",
      " |-- PrgDist_per90: double (nullable = true)\n",
      " |-- Carries_Att_3rd_per90: double (nullable = true)\n",
      " |-- Carries_Att_Pen_per90: double (nullable = true)\n",
      " |-- fail_To_Gain_Control_per90: double (nullable = true)\n",
      " |-- Loss_Control_Tackle_per90: double (nullable = true)\n",
      " |-- Sh_per90: double (nullable = true)\n",
      " |-- SoT_per90: double (nullable = true)\n",
      " |-- xG_per90: double (nullable = true)\n",
      " |-- npxG_per90: double (nullable = true)\n",
      " |-- GA_per90: double (nullable = true)\n",
      " |-- SoTA_per90: double (nullable = true)\n",
      " |-- PKatt_per90: double (nullable = true)\n",
      " |-- PSxG_per90: double (nullable = true)\n",
      " |-- PSxG/SoT_per90: double (nullable = true)\n",
      " |-- PSxG+/-_per90: double (nullable = true)\n",
      " |-- Crosses_Opp_per90: double (nullable = true)\n",
      " |-- #OPA_per90: double (nullable = true)\n",
      " |-- Effective_Pass_Short: float (nullable = true)\n",
      " |-- Effective_Pass_Medium: float (nullable = true)\n",
      " |-- Effective_Pass_Long: float (nullable = true)\n",
      " |-- Effective_Tkl: float (nullable = true)\n",
      " |-- Effective_Drib: float (nullable = true)\n",
      " |-- Effective_Tackles_vsDrib: float (nullable = true)\n",
      " |-- Effective_Penalty_Saves: float (nullable = true)\n",
      " |-- Effective_Cross_stop: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"data/unidos/merge_jugadores.parquet\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5fb8be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4969"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0bf52bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Player: string (nullable = true)\n",
      " |-- Squad: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- Pos: string (nullable = true)\n",
      " |-- Min: float (nullable = true)\n",
      " |-- Value: float (nullable = true)\n",
      " |-- performance_score: double (nullable = true)\n",
      " |-- penalty_score: double (nullable = true)\n",
      " |-- adjusted_score: double (nullable = true)\n",
      " |-- Evaluated_Position: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.parquet(\"data/final/merge_jugadores.parquet\")\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f358c574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4969"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5ef98ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "\n",
    "df_union = df1.toPandas()\n",
    "\n",
    "df_union.to_csv(\"data/performance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a431c545",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rank\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Definir una ventana por Posición, ordenada por adjusted_score descendente\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m window_spec \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPos\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(\u001b[43mcol\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjusted_score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\n\u001b[0;32m      6\u001b[0m df_2024_2025 \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeason\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-2025\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Aplicar ranking\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'col' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "# Definir una ventana por Posición, ordenada por adjusted_score descendente\n",
    "window_spec = Window.partitionBy(\"Pos\").orderBy(col(\"adjusted_score\").desc())\n",
    "df_2024_2025 = df1.filter(col(\"Season\") == \"2024-2025\")\n",
    "# Aplicar ranking\n",
    "df_ranked = df1.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Filtrar los mejores (rank = 1) por posición\n",
    "df_top_by_position = df_ranked.filter(col(\"rank\") == 1)\n",
    "\n",
    "# Mostrar resultados\n",
    "df_top_by_position.select(\n",
    "    \"Pos\", \"Player\", \"Squad\", \"Season\", \"adjusted_score\", \"performance_score\", \"penalty_score\", \"Value\"\n",
    ").orderBy(\"Pos\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1805740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, trim, round, when\n",
    "from pyspark.sql.types import FloatType\n",
    "import builtins\n",
    "position_metrics = {\n",
    "    \n",
    "    \"Goalkeeper\": {\"Recov\": 0.3, \"Clearences\": 0.2, \"Pass_cmp%\": 0.3, \"GA\": 0.3, \"SoTA\": 0.4, \"Save%\": 0.6, \"CS%\": 0.3, \"Effective_Penalty_Saves\": 0.2,\n",
    "        \"PSxG/SoT\": 0.6, \"PSxG\": 0.6, \"PSxG+/-\": 0.5, \"Launch%\": 0.2, \"Effective_Cross_stop\": 0.5, \"#OPA\": 0.4\n",
    "    },\n",
    "    \"Centre-Back\": {\n",
    "        \"Int\": 0.4, \"Blocks\": 0.3, \"Clearences\": 0.5, \"Aerialwon%\": 0.6,\n",
    "        \"Touch_Def_3rd\": 0.2, \"Tack_Def_3rd\": 0.4, \"PrgC\": 0.1, \"PrgP\": 0.2, \"Effective_Pass_Short\": 0.2,\n",
    "        \"Effective_Pass_Medium\": 0.2, \"Effective_Pass_Long\": 0.3,\n",
    "        \"Effective_Tkl\": 0.5, \"Block_Shots\": 0.4, \"Effective_Tackles_vsDrib\": 0.3,\n",
    "        \"PrgDist\": 0.1\n",
    "    },\n",
    "    \"Left-Back\":{ \n",
    "        \"PrgC\": 0.3, \"PrgP\": 0.2, \"Crosses\": 0.4, \"Recov\": 0.4, \"Tack_Def_3rd\": 0.4, \"Effective_Tkl\": 0.5, \"Int\": 0.5,\n",
    "        \"Block_Shots\": 0.4, \"Block_Pass\": 0.4, \"Clearences\": 0.5, \"Effective_Pass_Short\": 0.2,\n",
    "        \"Effective_Pass_Medium\": 0.3, \"Effective_Pass_Long\": 0.1, \"xA\": 0.2,\n",
    "        \"A-xAG\": 0.2, \"CrsPA\": 0.3, \"Touch_Def_3rd\": 0.3, \"Touch_Att_3rd\": 0.4, \"Effective_Drib\": 0.2,\n",
    "        \"PrgDist\": 0.4, \"Carries_Att_3rd\": 0.2, \"Touch_Live\": 0.3, \"Effective_Tackles_vsDrib\": 0.5\n",
    "    },\n",
    "    \"Right-Back\": {\n",
    "        \"PrgC\": 0.3, \"PrgP\": 0.2, \"Crosses\": 0.4, \"Recov\": 0.4, \"Tack_Def_3rd\": 0.4, \"Effective_Tkl\": 0.5, \"Int\": 0.5,\n",
    "        \"Block_Shots\": 0.4, \"Block_Pass\": 0.4, \"Clearences\": 0.5, \"Effective_Pass_Short\": 0.2,\n",
    "        \"Effective_Pass_Medium\": 0.3, \"Effective_Pass_Long\": 0.1, \"xA\": 0.2,\n",
    "        \"A-xAG\": 0.2, \"CrsPA\": 0.3, \"Touch_Def_3rd\": 0.3, \"Touch_Att_3rd\": 0.4, \"Effective_Drib\": 0.2,\n",
    "        \"PrgDist\": 0.4, \"Carries_Att_3rd\": 0.2, \"Touch_Live\": 0.3, \"Effective_Tackles_vsDrib\": 0.5\n",
    "    },\n",
    "    \"Defensive Midfield\": {\n",
    "        \"Int\": 0.4, \"Blocks\": 0.5, \"Clearences\": 0.3, \"Aerialwon%\": 0.3, \"Touch_Def_3rd\": 0.2,\n",
    "        \"Touch_Mid_3rd\": 0.4, \"Tack_Def_3rd\": 0.4, \"Effective_Pass_Short\": 0.6,\n",
    "        \"Effective_Pass_Medium\": 0.5, \"Effective_Pass_Long\": 0.5, \"PrgC\": 0.2, \"PrgP\": 0.3,\n",
    "        \"Effective_Tkl\": 0.5, \"Block_Shots\": 0.3,\"PrgDist\": 0.2, \"Recov\": 0.4,\n",
    "        \"Tack_Mid_3rd\": 0.3, \"Touch_Live\": 0.4\n",
    "   } ,\n",
    "    \"Central Midfield\": {\n",
    "        \"Ast\": 0.2, \"PrgC\": 0.3, \"PrgP\": 0.4, \"Recov\": 0.3, \"Aerialwon%\": 0.2, \"Tack_Def_3rd\": 0.3, \"Tack_Mid_3rd\": 0.4,\n",
    "        \"Tack_Att_3rd\": 0.4, \"Effective_Tkl\": 0.4, \"Int\": 0.4, \"Block_Pass\": 0.3,\"Effective_Pass_Short\": 0.6,\n",
    "        \"Effective_Pass_Medium\": 0.6, \"Effective_Pass_Long\": 0.5, \"xAG\":0.2, \"xA\": 0.2, \"A-xAG\": 0.2, \"Pass_cmp_Att_3rd\": 0.3, \"PPA\": 0.4,\n",
    "        \"Touch_Def_3rd\": 0.3, \"Touch_Mid_3rd\": 0.4, \"Touch_Att_3rd\": 0.3, \"Effective_Tackles_vsDrib\": 0.2,\n",
    "        \"PrgDist\": 0.3, \"Carries_Att_3rd\": 0.4, \"Touch_Live\": 0.2, \"Effective_Drib\": 0.2\n",
    "    },\n",
    "    \"Attacking Midfield\": {\n",
    "        \"Ast\": 0.3, \"PrgC\": 0.3, \"PrgP\": 0.4, \"PrgR\": 0.4, \"Tack_Att_3rd\": 0.3, \"Tack_Mid_3rd\": 0.2, \"Block_Pass\": 0.2,\n",
    "        \"Effective_Pass_Short\": 0.5, \"Effective_Pass_Medium\": 0.5, \"Effective_Pass_Long\": 0.3, \"xAG\": 0.5, \"xA\": 0.4, \"A-xAG\":0.3, \"Pass_cmp_Att_3rd\": 0.5, \"PPA\": 0.4,\n",
    "        \"Touch_Mid_3rd\": 0.3, \"Touch_Att_3rd\": 0.4, \"Touch_Att_Pen\": 0.2, \"Effective_Drib\": 0.3,\n",
    "        \"Effective_Tackles_vsDrib\": 0.1, \"PrgDist\": 0.2, \"Carries_Att_3rd\": 0.3, \"Carries_Att_Pen\": 0.4,\n",
    "        \"Sh\": 0.2, \"SoT\": 0.3, \"G/Sh\": 0.2, \"Touch_Live\": 0.2\n",
    "    },\n",
    "    \"Right Midfield\": {\n",
    "        \"Gls\": 0.1, \"Ast\": 0.2, \"PrgC\": 0.4, \"PrgP\": 0.3, \"PrgR\": 0.3, \"Crosses\": 0.4, \"Tack_Mid_3rd\": 0.3, \"Tack_Att_3rd\": 0.3,\n",
    "        \"Effective_Tkl\": 0.2, \"Block_Pass\": 0.2, \"Effective_Pass_Short\": 0.4,\n",
    "        \"Effective_Pass_Medium\": 0.4, \"Effective_Pass_Long\": 0.2, \"xAG\": 0.3, \"xA\": 0.3, \"A-xAG\": 0.3, \"Pass_cmp_Att_3rd\": 0.4, \"PPA\": 0.5, \"CrsPA\": 0.5,\n",
    "        \"Touch_Mid_3rd\": 0.3, \"Touch_Att_3rd\": 0.4, \"Effective_Drib\": 0.5, \"Effective_Tackles_vsDrib\": 0.2, \"PrgDist\": 0.4, \"Carries_Att_3rd\": 0.5, \"Carries_Att_Pen\": 0.4,\n",
    "        \"Sh\": 0.2, \"SoT\": 0.2, \"G/Sh\": 0.2\n",
    "   } ,\n",
    "    \"Left Midfield\": {\n",
    "        \"Gls\": 0.1, \"Ast\": 0.2, \"PrgC\": 0.4, \"PrgP\": 0.3, \"PrgR\": 0.3, \"Crosses\": 0.4, \"Tack_Mid_3rd\": 0.3, \"Tack_Att_3rd\": 0.3,\n",
    "        \"Effective_Tkl\": 0.2, \"Block_Pass\": 0.2, \"Effective_Pass_Short\": 0.4,\n",
    "        \"Effective_Pass_Medium\": 0.4, \"Effective_Pass_Long\": 0.2, \"xAG\": 0.3, \"xA\": 0.3, \"A-xAG\": 0.4, \"Pass_cmp_Att_3rd\": 0.4, \"PPA\": 0.5, \"CrsPA\": 0.5,\n",
    "        \"Touch_Mid_3rd\": 0.3, \"Touch_Att_3rd\": 0.4, \"Effective_Drib\": 0.5, \"Effective_Tackles_vsDrib\": 0.2, \"PrgDist\": 0.4, \"Carries_Att_3rd\": 0.5, \"Carries_Att_Pen\": 0.4,\n",
    "        \"Sh\": 0.2, \"SoT\": 0.2, \"G/Sh\": 0.2\n",
    "    },\n",
    "    \"Left Winger\": {\n",
    "        \"Gls\": 0.2, \"Ast\": 0.3, \"PrgC\": 0.5, \"PrgP\": 0.4, \"PrgR\": 0.4, \"Crosses\": 0.3, \"Tack_Att_3rd\": 0.4, \"Effective_Tkl\": 0.2,\n",
    "        \"Block_Pass\": 0.1, \"Effective_Pass_Short\": 0.3,\n",
    "        \"Effective_Pass_Medium\": 0.3, \"Effective_Pass_Long\": 0.2, \"PPA\": 0.5, \"CrsPA\": 0.6, \"Touch_Att_3rd\": 0.5,\n",
    "        \"Touch_Att_Pen\": 0.5, \"Effective_Drib\": 0.6, \"Effective_Tackles_vsDrib\": 0.1,\n",
    "        \"PrgDist\": 0.3, \"Carries_Att_3rd\": 0.4, \"Carries_Att_Pen\": 0.5, \"xAG\": 0.3, \"xA\": 0.3,\n",
    "        \"Sh\": 0.4, \"SoT\": 0.3, \"G/Sh\": 0.3, \"xG\": 0.5, \"npxG\": 0.3, \"G-xG\": 0.3\n",
    "    },\n",
    "    \"Right Winger\": {\n",
    "        \"Gls\": 0.2, \"Ast\": 0.3, \"PrgC\": 0.5, \"PrgP\": 0.4, \"PrgR\": 0.4, \"Crosses\": 0.3, \"Tack_Att_3rd\": 0.4, \"Effective_Tkl\": 0.2,\n",
    "        \"Block_Pass\": 0.1, \"Effective_Pass_Short\": 0.3,\n",
    "        \"Effective_Pass_Medium\": 0.3, \"Effective_Pass_Long\": 0.2, \"PPA\": 0.5, \"CrsPA\": 0.6, \"Touch_Att_3rd\": 0.5,\n",
    "        \"Touch_Att_Pen\": 0.5, \"Effective_Drib\": 0.6, \"Effective_Tackles_vsDrib\": 0.1,\n",
    "        \"PrgDist\": 0.3, \"Carries_Att_3rd\": 0.4, \"Carries_Att_Pen\": 0.5, \"xAG\": 0.3, \"xA\": 0.3,\n",
    "        \"Sh\": 0.4, \"SoT\": 0.3, \"G/Sh\": 0.3, \"xG\": 0.5, \"npxG\": 0.3, \"G-xG\": 0.3\n",
    "    },\n",
    "    \"Second Striker\": {\n",
    "        \"Gls\": 0.3, \"Ast\": 0.3, \"PrgC\": 0.4, \"PrgP\": 0.5, \"PrgR\": 0.6, \"Tack_Att_3rd\": 0.4, \"Block_Pass\": 0.1,\n",
    "        \"Effective_Pass_Short\": 0.4,\n",
    "        \"Effective_Pass_Medium\": 0.3, \"Effective_Pass_Long\": 0.2, \"xAG\": 0.5, \"xA\": 0.4,\n",
    "        \"Pass_cmp_Att_3rd\": 0.4, \"PPA\": 0.5, \"Touch_Mid_3rd\": 0.3, \"Touch_Att_3rd\": 0.6, \"Touch_Att_Pen\": 0.5,\n",
    "        \"Effective_Drib\":0.4, \"Effective_Tackles_vsDrib\": 0.1, \"PrgDist\": 0.3,\n",
    "        \"Carries_Att_3rd\": 0.3, \"Carries_Att_Pen\": 0.4,\n",
    "        \"Sh\": 0.3, \"SoT\": 0.6, \"G/Sh\": 0.5, \"xG\": 0.6, \"npxG\": 0.4, \"G-xG\": 0.5, \"Touch_Live\": 0.2, \"Effective_Tkl\": 0.1\n",
    "    },\n",
    "    \"Centre-Forward\": {\n",
    "        \"Gls\":0.3, \"Ast\":0.2, \"PrgR\": 0.3, \"Aerialwon%\": 0.5, \"Tack_Att_3rd\": 0.5, \"Effective_Tkl\": 0.4, \"Block_Pass\": 0.2,\n",
    "        \"Effective_Pass_Short\": 0.3,\n",
    "        \"Effective_Pass_Medium\": 0.2, \"Effective_Pass_Long\": 0.1, \"xAG\": 0.4,\n",
    "        \"xA\": 0.4, \"Pass_cmp_Att_3rd\": 0.4, \"Touch_Att_3rd\": 0.5, \"Touch_Mid_3rd\": 0.2, \"Touch_Att_Pen\": 0.6,\n",
    "        \"Carries_Att_Pen\": 0.5, \"Effective_Drib\": 0.4, \n",
    "        \"Sh\": 0.5, \"SoT\": 0.6, \"G/Sh\": 0.5, \"xG\": 0.7, \"npxG\": 0.5, \"G-xG\": 0.4\n",
    "    }\n",
    "}\n",
    "def normalize_weights_by_position(position_metrics):\n",
    "    normalized = {}\n",
    "    for position, weights in position_metrics.items():\n",
    "        total = sum(weights.values())\n",
    "        if total == 0:\n",
    "            raise ValueError(f\"La posición '{position}' tiene una suma de pesos igual a cero.\")\n",
    "        normalized[position] = [\n",
    "            (metric, builtins.round(weight / total, 4)) for metric, weight in weights.items()\n",
    "        ]\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "be60dcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizar pesos...\n",
      "calculando el performance score...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.sql.functions import col, lit, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from functools import reduce\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.sql.functions import col, lit\n",
    "from functools import reduce\n",
    "\n",
    "def calcular_performance_score_con_pesos(df, position_metrics_normalized):\n",
    "    def procesar_posicion_x_temporada(df, pos, metric_weight_tuples):\n",
    "        available_columns = df.columns\n",
    "        seasons = [row[\"Season\"] for row in df.select(\"Season\").distinct().collect()]\n",
    "        df_pos_season_list = []\n",
    "\n",
    "        for season in seasons:\n",
    "            df_season = df.filter((col(\"Pos\") == pos) & (col(\"Season\") == season))\n",
    "\n",
    "            metrics = [m for m, _ in metric_weight_tuples if m in available_columns]\n",
    "            weights = [w for m, w in metric_weight_tuples if m in available_columns]\n",
    "            metric_weight_tuples_filtered = list(zip(metrics, weights))\n",
    "\n",
    "            if not metric_weight_tuples_filtered:\n",
    "                continue\n",
    "\n",
    "            df_season = df_season.fillna(0, metrics)\n",
    "\n",
    "            assembler = VectorAssembler(inputCols=metrics, outputCol=\"features_vec\")\n",
    "            df_season = assembler.transform(df_season)\n",
    "\n",
    "            scaler = MinMaxScaler(inputCol=\"features_vec\", outputCol=\"scaled_features\")\n",
    "            scaler_model = scaler.fit(df_season)\n",
    "            df_season = scaler_model.transform(df_season)\n",
    "\n",
    "            # ✅ convertir el vector a array usando librería oficial de PySpark ML\n",
    "            df_season = df_season.withColumn(\"scaled_array\", vector_to_array(\"scaled_features\"))\n",
    "\n",
    "            # calcular columnas ponderadas\n",
    "            weighted_cols = []\n",
    "            for i, (metric, weight) in enumerate(metric_weight_tuples_filtered):\n",
    "                weighted_col_name = f\"{metric}_weighted\"\n",
    "                df_season = df_season.withColumn(weighted_col_name, col(\"scaled_array\")[i] * lit(weight))\n",
    "                weighted_cols.append(col(weighted_col_name))\n",
    "\n",
    "            if weighted_cols:\n",
    "                df_season = df_season.withColumn(\"performance_score\", sum(weighted_cols))\n",
    "            else:\n",
    "                df_season = df_season.withColumn(\"performance_score\", lit(0.0))\n",
    "\n",
    "            # eliminar columnas temporales\n",
    "            cols_to_drop = [\"features_vec\", \"scaled_features\", \"scaled_array\"] + \\\n",
    "                           [f\"{metric}_weighted\" for metric, _ in metric_weight_tuples_filtered]\n",
    "            df_season = df_season.drop(*cols_to_drop)\n",
    "\n",
    "            df_season = df_season.withColumn(\"Evaluated_Position\", lit(pos))\n",
    "            df_pos_season_list.append(df_season)\n",
    "\n",
    "        return reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), df_pos_season_list) if df_pos_season_list else df.limit(0)\n",
    "\n",
    "    scored_dfs = []\n",
    "    for pos, metric_weight_tuples in position_metrics_normalized.items():\n",
    "        if metric_weight_tuples:\n",
    "            scored_df = procesar_posicion_x_temporada(df, pos, metric_weight_tuples)\n",
    "            if scored_df.count() > 0:\n",
    "                scored_dfs.append(scored_df)\n",
    "\n",
    "    return reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), scored_dfs) if scored_dfs else df.limit(0)\n",
    "\n",
    "\n",
    "print(\"normalizar pesos...\")\n",
    "position_metrics_normalized = normalize_weights_by_position(position_metrics)\n",
    "print(\"calculando el performance score...\")\n",
    "df = calcular_performance_score_con_pesos(df, position_metrics_normalized)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "663a8685",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o40286.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 940.0 failed 1 times, most recent failure: Lost task 0.0 in stage 940.0 (TID 837) (LAPTOP-SNOCNEUP.mshome.net executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:200)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:110)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:123)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonEvaluatorFactory.$anonfun$evaluate$2(BatchEvalPythonExec.scala:96)\r\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:88)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:333)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeNextInputToStream(PythonRunner.scala:906)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:844)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor209.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:200)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:110)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:123)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonEvaluatorFactory.$anonfun$evaluate$2(BatchEvalPythonExec.scala:96)\r\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:88)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:333)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeNextInputToStream(PythonRunner.scala:906)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:844)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df\u001b[38;5;241m.\u001b[39mrdd, schema\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mschema)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf_clean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o40286.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 940.0 failed 1 times, most recent failure: Lost task 0.0 in stage 940.0 (TID 837) (LAPTOP-SNOCNEUP.mshome.net executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:200)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:110)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:123)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonEvaluatorFactory.$anonfun$evaluate$2(BatchEvalPythonExec.scala:96)\r\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:88)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:333)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeNextInputToStream(PythonRunner.scala:906)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:844)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor209.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:200)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:110)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:123)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonEvaluatorFactory.$anonfun$evaluate$2(BatchEvalPythonExec.scala:96)\r\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:88)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:333)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeNextInputToStream(PythonRunner.scala:906)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:844)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "df_clean = spark.createDataFrame(df.rdd, schema=df.schema)\n",
    "df_clean.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b95e4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Player', 'Squad', 'Season', 'Competition', 'Born', 'MP', 'Starts', 'Min', 'Gls', 'Ast', 'YellowC', 'RedC', 'PrgC', 'PrgP', 'PrgR', 'Fls', 'Off', 'Crosses', 'Recov', 'Aerialwon%', 'Tack_Def_3rd', 'Tack_Mid_3rd', 'Tack_Att_3rd', 'Tkl%', 'Tkl', 'TklW', 'Int', 'Blocks', 'Block_Shots', 'Block_Pass', 'Clearences', 'Err', 'Pass_cmp', 'Pass_cmp%', 'Pass_Short', 'Pass_cmp_Short%', 'Pass_Medium', 'Pass_cmp_Medium%', 'Pass_Long', 'Pass_cmp_Long%', 'xAG', 'xA', 'A-xAG', 'Pass_cmp_Att_3rd', 'PPA', 'CrsPA', 'Touch_Def_3rd', 'Touch_Mid_3rd', 'Touch_Att_3rd', 'Touch_Att_Pen', 'Touch_Live', 'drib_Att', 'drib_Succ%', 'Tckl_Drib', 'Tckl_Drib%', 'PrgDist', 'Carries_Att_3rd', 'Carries_Att_Pen', 'fail_To_Gain_Control', 'Loss_Control_Tackle', 'Sh', 'SoT', 'G/Sh', 'xG', 'npxG', 'npxG/Sh', 'G-xG', 'Pos', 'Height', 'Value', 'GA', 'SoTA', 'Save%', 'CS%', 'PKatt', 'P_Save%', 'PSxG', 'PSxG/SoT', 'PSxG+/-', 'Launch%', 'Crosses_Opp', 'Crosses_stp%', '#OPA', 'Gls_per90', 'Ast_per90', 'PrgC_per90', 'PrgP_per90', 'PrgR_per90', 'Fls_per90', 'Off_per90', 'Crosses_per90', 'Recov_per90', 'Tack_Def_3rd_per90', 'Tack_Mid_3rd_per90', 'Tack_Att_3rd_per90', 'Tkl_per90', 'TklW_per90', 'Int_per90', 'Blocks_per90', 'Block_Shots_per90', 'Block_Pass_per90', 'Clearences_per90', 'Err_per90', 'Pass_cmp_per90', 'Pass_Short_per90', 'Pass_Medium_per90', 'Pass_Long_per90', 'xAG_per90', 'xA_per90', 'A-xAG_per90', 'Pass_cmp_Att_3rd_per90', 'PPA_per90', 'CrsPA_per90', 'Touch_Def_3rd_per90', 'Touch_Mid_3rd_per90', 'Touch_Att_3rd_per90', 'Touch_Att_Pen_per90', 'Touch_Live_per90', 'drib_Att_per90', 'Tckl_Drib_per90', 'PrgDist_per90', 'Carries_Att_3rd_per90', 'Carries_Att_Pen_per90', 'fail_To_Gain_Control_per90', 'Loss_Control_Tackle_per90', 'Sh_per90', 'SoT_per90', 'xG_per90', 'npxG_per90', 'GA_per90', 'SoTA_per90', 'PKatt_per90', 'PSxG_per90', 'PSxG/SoT_per90', 'PSxG+/-_per90', 'Crosses_Opp_per90', '#OPA_per90', 'Effective_Pass_Short', 'Effective_Pass_Medium', 'Effective_Pass_Long', 'Effective_Tkl', 'Effective_Drib', 'Effective_Tackles_vsDrib', 'Effective_Penalty_Saves', 'Effective_Cross_stop', 'penalty_YellowC', 'penalty_RedC', 'penalty_Fls', 'penalty_Err', 'penalty_Loss_Control_Tackle', 'penalty_fail_To_Gain_Control', 'penalty_Off', 'penalty_score', 'performance_score', 'Evaluated_Position']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3453da12",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o40125.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 938.0 failed 1 times, most recent failure: Lost task 0.0 in stage 938.0 (TID 835) (LAPTOP-SNOCNEUP.mshome.net executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:200)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:110)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:123)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonEvaluatorFactory.$anonfun$evaluate$2(BatchEvalPythonExec.scala:96)\r\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:1240)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor209.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:200)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:110)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:123)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonEvaluatorFactory.$anonfun$evaluate$2(BatchEvalPythonExec.scala:96)\r\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:1240)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o40125.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 938.0 failed 1 times, most recent failure: Lost task 0.0 in stage 938.0 (TID 835) (LAPTOP-SNOCNEUP.mshome.net executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:200)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:110)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:123)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonEvaluatorFactory.$anonfun$evaluate$2(BatchEvalPythonExec.scala:96)\r\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:1240)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor209.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:200)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:110)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:123)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonEvaluatorFactory.$anonfun$evaluate$2(BatchEvalPythonExec.scala:96)\r\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:1240)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9efa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+---------------+--------------+-------------------+\n",
      "|              Player|               Pos|          Squad|   Competition|  performance_score|\n",
      "+--------------------+------------------+---------------+--------------+-------------------+\n",
      "|       florian wirtz|Attacking Midfield|     leverkusen|    bundesliga| 0.4698577307771978|\n",
      "|               pedri|  Central Midfield|      barcelona|        laliga| 0.4287534923294778|\n",
      "|          mario gila|       Centre-Back|          lazio|       serie-a|0.32671365296692373|\n",
      "|     shumaira mheuka|    Centre-Forward|        chelsea|premier-league|             0.4063|\n",
      "|      joshua kimmich|Defensive Midfield|  bayern munich|    bundesliga| 0.5666147573187089|\n",
      "|           pau lopez|        Goalkeeper|         girona|        laliga|0.47719619945274167|\n",
      "|    marcus tavernier|     Left Midfield|    bournemouth|premier-league| 0.5959664719230389|\n",
      "|         jeremy doku|       Left Winger|manchester city|premier-league| 0.3131525041861526|\n",
      "|maximilian mittel...|         Left-Back|      stuttgart|    bundesliga|0.39897133413048713|\n",
      "|       juan cuadrado|    Right Midfield|       atalanta|       serie-a| 0.6739622561520988|\n",
      "|        lamine yamal|      Right Winger|      barcelona|        laliga| 0.5085580010780223|\n",
      "|        caleb kporha|        Right-Back| crystal palace|premier-league| 0.3800329521224277|\n",
      "|     ademola lookman|    Second Striker|       atalanta|       serie-a| 0.5057807420377188|\n",
      "+--------------------+------------------+---------------+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "def obtener_mejores_jugadores_por_posicion(df, temporada=\"2024-2025\"):\n",
    "    # Filtrar temporada y asegurarse de que performance_score no sea nulo\n",
    "    df_filtrado = df.filter((col(\"Season\") == temporada) & (col(\"performance_score\").isNotNull()))\n",
    "\n",
    "    # Crear ventana por Evaluated_Position y ordenar por performance_score descendente\n",
    "    window_pos = Window.partitionBy(\"Evaluated_Position\").orderBy(col(\"performance_score\").desc())\n",
    "\n",
    "    # Asignar ranking y quedarte con el top 1 por posición\n",
    "    df_top = df_filtrado.withColumn(\"rank\", row_number().over(window_pos)) \\\n",
    "                        .filter(col(\"rank\") == 1) \\\n",
    "                        .drop(\"rank\")\n",
    "\n",
    "    return df_top\n",
    "\n",
    "obtener_mejores_jugadores_por_posicion(df1).select(\"Player\", \"Pos\", \"Squad\", \"Competition\", \"performance_score\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbf54494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Player',\n",
       " 'Squad',\n",
       " 'Season',\n",
       " 'Competition',\n",
       " 'Pos',\n",
       " 'Min',\n",
       " 'Value',\n",
       " 'performance_score',\n",
       " 'penalty_score',\n",
       " 'adjusted_score',\n",
       " 'Evaluated_Position']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efecb164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+---------------+-----------+-------------------+--------------------+-------------------+\n",
      "|             Player|           Pos|          Squad|Competition|  performance_score|       penalty_score|     adjusted_score|\n",
      "+-------------------+--------------+---------------+-----------+-------------------+--------------------+-------------------+\n",
      "|      kylian mbappe|Centre-Forward|    real madrid|     laliga| 0.2146943242814554|              0.0928|0.12189432428145541|\n",
      "|benedict hollerbach|Centre-Forward|   union berlin| bundesliga|0.15549765424037695|              0.1348|0.02069765424037695|\n",
      "|  antoine griezmann|Centre-Forward|atlético madrid|     laliga|0.15420179685048288|0.045700000000000005|0.10850179685048288|\n",
      "|         harry kane|Centre-Forward|  bayern munich| bundesliga| 0.1496455294043452|              0.0475| 0.1021455294043452|\n",
      "|     mohamed amoura|Centre-Forward|      wolfsburg| bundesliga| 0.1466798520285576| 0.07310000000000001|0.07357985202855759|\n",
      "+-------------------+--------------+---------------+-----------+-------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "def obtener_mejores_jugadores_por_posicion(df, position, temporada=\"2024-2025\"):\n",
    "    # Filtrar temporada y asegurarse de que performance_score no sea nulo\n",
    "    df_filtrado = df.filter((col(\"Season\") == temporada) & (col(\"performance_score\").isNotNull()))\n",
    "\n",
    "    # Crear ventana por Evaluated_Position y ordenar por performance_score descendente\n",
    "    window_pos = Window.partitionBy(\"pos\").orderBy(col(\"performance_score\").desc())\n",
    "\n",
    "    # Asignar ranking y quedarte con el top 1 por posición\n",
    "    df_top = df_filtrado.withColumn(\"rank\", row_number().over(window_pos)) \\\n",
    "                        .filter(col(\"rank\") < 11) \\\n",
    "                        .filter(col(\"pos\") == position)\\\n",
    "                        .filter(col(\"Min\") > 600)\\\n",
    "                        .drop(\"rank\")\n",
    "\n",
    "    return df_top\n",
    "\n",
    "obtener_mejores_jugadores_por_posicion(df1, \"Centre-Forward\").select(\"Player\", \"Pos\", \"Squad\", \"Competition\", \"performance_score\", \"penalty_score\", \"adjusted_score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fafe87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+---------------+-----------+-------------------+--------------------+-------------------+\n",
      "|           Player|           Pos|          Squad|Competition|  performance_score|       penalty_score|     adjusted_score|\n",
      "+-----------------+--------------+---------------+-----------+-------------------+--------------------+-------------------+\n",
      "|    kylian mbappe|Centre-Forward|    real madrid|     laliga| 0.2146943242814554|              0.0928|0.12189432428145541|\n",
      "|antoine griezmann|Centre-Forward|atlético madrid|     laliga|0.15420179685048288|0.045700000000000005|0.10850179685048288|\n",
      "|       harry kane|Centre-Forward|  bayern munich| bundesliga| 0.1496455294043452|              0.0475| 0.1021455294043452|\n",
      "|       iago aspas|Centre-Forward|     celta vigo|     laliga|0.14606093587227076|              0.0522|0.09386093587227076|\n",
      "|   marvin ducksch|Centre-Forward|  werder bremen| bundesliga|0.11710087121811101|              0.0378|0.07930087121811101|\n",
      "|    gerard moreno|Centre-Forward|     villarreal|     laliga|0.13662306684814782|0.058499999999999996|0.07812306684814782|\n",
      "|   julian alvarez|Centre-Forward|atlético madrid|     laliga|0.14366076499109212|              0.0685|0.07516076499109212|\n",
      "+-----------------+--------------+---------------+-----------+-------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "def obtener_mejores_jugadores_por_posicion(df, position, temporada=\"2024-2025\"):\n",
    "    # Filtrar temporada y asegurarse de que performance_score no sea nulo\n",
    "    df_filtrado = df.filter((col(\"Season\") == temporada) & (col(\"adjusted_score\").isNotNull()))\n",
    "\n",
    "    # Crear ventana por Evaluated_Position y ordenar por performance_score descendente\n",
    "    window_pos = Window.partitionBy(\"pos\").orderBy(col(\"adjusted_score\").desc())\n",
    "\n",
    "    # Asignar ranking y quedarte con el top 1 por posición\n",
    "    df_top = df_filtrado.withColumn(\"rank\", row_number().over(window_pos)) \\\n",
    "                        .filter(col(\"rank\") < 11) \\\n",
    "                        .filter(col(\"pos\") == position)\\\n",
    "                        .filter(col(\"Min\") > 600)\\\n",
    "                        .drop(\"rank\")\n",
    "\n",
    "    return df_top\n",
    "\n",
    "obtener_mejores_jugadores_por_posicion(df1, \"Centre-Forward\").select(\"Player\", \"Pos\", \"Squad\", \"Competition\", \"performance_score\", \"penalty_score\", \"adjusted_score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "091e3b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Player: string (nullable = true)\n",
      " |-- Squad: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- Pos: string (nullable = true)\n",
      " |-- Value: float (nullable = true)\n",
      " |-- performance_score: double (nullable = true)\n",
      " |-- penalty_score: double (nullable = true)\n",
      " |-- adjusted_score: double (nullable = true)\n",
      " |-- Evaluated_Position: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.parquet(\"data/final/merge_jugadores.parquet\")\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3e1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1048b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+---------+-----------+----------+------+-----+------------------+-------------+------------------+------------------+\n",
      "|          Player|      Squad|   Season|Competition|       Pos|   Min|Value| performance_score|penalty_score|    adjusted_score|Evaluated_Position|\n",
      "+----------------+-----------+---------+-----------+----------+------+-----+------------------+-------------+------------------+------------------+\n",
      "|thibaut courtois|real madrid|2023-2024|     laliga|Goalkeeper| 332.0|2.8E7|0.3738839432952276|          0.0|0.3738839432952276|        Goalkeeper|\n",
      "|thibaut courtois|real madrid|2024-2025|     laliga|Goalkeeper|2700.0|2.0E7|0.3363258408796188|       0.0015|0.3348258408796188|        Goalkeeper|\n",
      "|thibaut courtois|real madrid|2022-2023|     laliga|Goalkeeper|2790.0|4.5E7|0.4346668970958383|       0.0021|0.4325668970958383|        Goalkeeper|\n",
      "+----------------+-----------+---------+-----------+----------+------+-----+------------------+-------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(col(\"Player\") == \"thibaut courtois\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a45d14cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+---------+--------------+------------------+------+---------+-------------------+--------------------+-------------------+------------------+\n",
      "|           Player|          Squad|   Season|   Competition|               Pos|   Min|    Value|  performance_score|       penalty_score|     adjusted_score|Evaluated_Position|\n",
      "+-----------------+---------------+---------+--------------+------------------+------+---------+-------------------+--------------------+-------------------+------------------+\n",
      "|    juan cuadrado|       atalanta|2024-2025|       serie-a|    Right Midfield| 832.0|1000000.0| 0.6739622561520988|0.046400000000000004| 0.6275622561520988|    Right Midfield|\n",
      "|   joshua kimmich|  bayern munich|2024-2025|    bundesliga|Defensive Midfield|2847.0|    4.5E7| 0.5666147573187089|0.020200000000000003| 0.5464147573187089|Defensive Midfield|\n",
      "| marcus tavernier|    bournemouth|2024-2025|premier-league|     Left Midfield|1939.0|    1.8E7| 0.5959664719230389| 0.08080000000000001| 0.5151664719230389|     Left Midfield|\n",
      "|davide zappacosta|       atalanta|2024-2025|       serie-a|    Right Midfield|2067.0|5000000.0| 0.5267487825414628|              0.0389|0.48784878254146286|    Right Midfield|\n",
      "|  raoul bellanova|       atalanta|2024-2025|       serie-a|    Right Midfield|2402.0|    2.4E7| 0.5095999181512105|              0.0247|0.48489991815121053|    Right Midfield|\n",
      "|  marcos llorente|atlético madrid|2024-2025|        laliga|    Right Midfield|2513.0|    2.5E7|0.49736140954458535|0.027000000000000003|0.47036140954458533|    Right Midfield|\n",
      "|    robin zentner|       mainz 05|2024-2025|    bundesliga|        Goalkeeper|2880.0|4000000.0| 0.4554211702517768|              0.0074| 0.4480211702517768|        Goalkeeper|\n",
      "|   manuel lazzari|          lazio|2024-2025|       serie-a|    Right Midfield|1406.0|3500000.0|0.47648981630654047|               0.034| 0.4424898163065405|    Right Midfield|\n",
      "|   frederik rnnow|   union berlin|2024-2025|    bundesliga|        Goalkeeper|2520.0|3000000.0| 0.4450480747274903|0.003900000000000...| 0.4411480747274903|        Goalkeeper|\n",
      "| manuel locatelli|       juventus|2024-2025|       serie-a|Defensive Midfield|2822.0|    3.0E7| 0.4654346471441756|              0.0279| 0.4375346471441756|Defensive Midfield|\n",
      "+-----------------+---------------+---------+--------------+------------------+------+---------+-------------------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# Filtro mínimo de minutos por posición (ajustable)\n",
    "\n",
    "def top_10_por_posicion_temporada(df, posicion, temporada, variable_score):\n",
    "    min_minutos = 600\n",
    "\n",
    "    return (\n",
    "        df.filter((col(\"Season\") == temporada) & (col(\"Min\") >= min_minutos))\n",
    "          .orderBy(desc(variable_score))\n",
    "          .limit(10)\n",
    "    )\n",
    "top_10_por_posicion_temporada(df1, \"Goalkeeper\", \"2024-2025\", \"adjusted_score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f345b12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+---------+--------------+--------------+------+---------+-------------------+--------------------+-------------------+------------------+\n",
      "|           Player|          Squad|   Season|   Competition|           Pos|   Min|    Value|  performance_score|       penalty_score|     adjusted_score|Evaluated_Position|\n",
      "+-----------------+---------------+---------+--------------+--------------+------+---------+-------------------+--------------------+-------------------+------------------+\n",
      "|    kylian mbappe|    real madrid|2024-2025|        laliga|Centre-Forward|2907.0|    1.8E8| 0.2146943242814554|              0.0928|0.12189432428145541|    Centre-Forward|\n",
      "|antoine griezmann|atlético madrid|2024-2025|        laliga|Centre-Forward|2469.0|    1.8E7|0.15420179685048288|0.045700000000000005|0.10850179685048288|    Centre-Forward|\n",
      "|       harry kane|  bayern munich|2024-2025|    bundesliga|Centre-Forward|2381.0|    7.5E7| 0.1496455294043452|              0.0475| 0.1021455294043452|    Centre-Forward|\n",
      "|       iago aspas|     celta vigo|2024-2025|        laliga|Centre-Forward|1745.0|2000000.0|0.14606093587227076|              0.0522|0.09386093587227076|    Centre-Forward|\n",
      "|   marvin ducksch|  werder bremen|2024-2025|    bundesliga|Centre-Forward|2418.0|6000000.0|0.11710087121811101|              0.0378|0.07930087121811101|    Centre-Forward|\n",
      "|    gerard moreno|     villarreal|2024-2025|        laliga|Centre-Forward| 721.0|4000000.0|0.13662306684814782|0.058499999999999996|0.07812306684814782|    Centre-Forward|\n",
      "|   julian alvarez|atlético madrid|2024-2025|        laliga|Centre-Forward|2509.0|    1.0E8|0.14366076499109212|              0.0685|0.07516076499109212|    Centre-Forward|\n",
      "|   mohamed amoura|      wolfsburg|2024-2025|    bundesliga|Centre-Forward|2467.0|    3.2E7| 0.1466798520285576| 0.07310000000000001|0.07357985202855759|    Centre-Forward|\n",
      "|   erling haaland|manchester city|2024-2025|premier-league|Centre-Forward|2736.0|    1.8E8| 0.1056749181821262|              0.0431| 0.0625749181821262|    Centre-Forward|\n",
      "|    ferran torres|      barcelona|2024-2025|        laliga|Centre-Forward|1106.0|    4.0E7|0.12043271176971836| 0.05789999999999999|0.06253271176971836|    Centre-Forward|\n",
      "+-----------------+---------------+---------+--------------+--------------+------+---------+-------------------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# Filtro mínimo de minutos por posición (ajustable)\n",
    "\n",
    "def top_10_por_posicion_temporada(df, posicion, temporada, variable_score):\n",
    "    min_minutos = 600\n",
    "\n",
    "    return (\n",
    "        df.filter((col(\"Pos\") == posicion) & (col(\"Season\") == temporada) & (col(\"Min\") >= min_minutos))\n",
    "          .orderBy(desc(variable_score))\n",
    "          .limit(10)\n",
    "    )\n",
    "top_10_por_posicion_temporada(df1, \"Centre-Forward\", \"2024-2025\", \"adjusted_score\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bb6bd38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\clientserver.py:527\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\clientserver.py:530\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    529\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending or receiving.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m\n\u001b[0;32m     12\u001b[0m     df_top \u001b[38;5;241m=\u001b[39m df_filtrado\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, row_number()\u001b[38;5;241m.\u001b[39mover(window_pos)) \\\n\u001b[0;32m     13\u001b[0m                         \u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \\\n\u001b[0;32m     14\u001b[0m                         \u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_top\n\u001b[1;32m---> 18\u001b[0m \u001b[43mobtener_mejores_jugadores_por_posicion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPos\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSquad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompetition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjusted_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperformance_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpenalty_score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m, in \u001b[0;36mobtener_mejores_jugadores_por_posicion\u001b[1;34m(df, temporada)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mobtener_mejores_jugadores_por_posicion\u001b[39m(df, temporada\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-2025\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Filtrar temporada y asegurarse de que performance_score no sea nulo\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     df_filtrado \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter((\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSeason\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m temporada) \u001b[38;5;241m&\u001b[39m (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperformance_score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull()))\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Crear ventana por Evaluated_Position y ordenar por performance_score descendente\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     window_pos \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluated_Position\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperformance_score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\pyspark\\sql\\utils.py:282\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\pyspark\\errors\\utils.py:269\u001b[0m, in \u001b[0;36m_with_origin.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m         set_current_origin(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetActiveSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spark \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\pyspark\\sql\\utils.py:357\u001b[0m, in \u001b[0;36mtry_remote_session_classmethod.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(SparkSession, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs[\u001b[38;5;241m1\u001b[39m:], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\pyspark\\sql\\session.py:746\u001b[0m, in \u001b[0;36mSparkSession.getActiveSession\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 746\u001b[0m     jSparkSessionClass \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_j_spark_session_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetActiveSession()\u001b[38;5;241m.\u001b[39misDefined():\n\u001b[0;32m    748\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m SparkSession\u001b[38;5;241m.\u001b[39m_should_update_active_session():\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\pyspark\\sql\\session.py:668\u001b[0m, in \u001b[0;36mSparkSession._get_j_spark_session_class\u001b[1;34m(jvm)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_j_spark_session_class\u001b[39m(jvm: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJVMView\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaClass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.sql.classic.SparkSession\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1752\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[1;32m-> 1752\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1057\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1055\u001b[0m         retry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while sending command.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1057\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     logging\u001b[38;5;241m.\u001b[39mexception(\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while sending command.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Universidad\\Master BDDE UCM\\TFM\\TFM\\venv\\lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "def obtener_mejores_jugadores_por_posicion(df, temporada=\"2024-2025\"):\n",
    "    # Filtrar temporada y asegurarse de que performance_score no sea nulo\n",
    "    df_filtrado = df.filter((col(\"Season\") == temporada) & (col(\"performance_score\").isNotNull()))\n",
    "\n",
    "    # Crear ventana por Evaluated_Position y ordenar por performance_score descendente\n",
    "    window_pos = Window.partitionBy(\"Evaluated_Position\").orderBy(col(\"performance_score\").desc())\n",
    "\n",
    "    # Asignar ranking y quedarte con el top 1 por posición\n",
    "    df_top = df_filtrado.withColumn(\"rank\", row_number().over(window_pos)) \\\n",
    "                        .filter(col(\"rank\") == 1) \\\n",
    "                        .drop(\"rank\")\n",
    "\n",
    "    return df_top\n",
    "\n",
    "obtener_mejores_jugadores_por_posicion(df1).select(\"Player\", \"Pos\", \"Squad\", \"Competition\", \"adjusted_score\", \"performance_score\", \"penalty_score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a610c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, min, max, when\n",
    "\n",
    "def ajustar_penalty_score(df):\n",
    "    # Ventana por posición y temporada\n",
    "    w = Window.partitionBy(\"Evaluated_Position\", \"Season\")\n",
    "\n",
    "    # Calcular mínimo y máximo por grupo\n",
    "    df = df.withColumn(\"penalty_min\", min(\"penalty_score\").over(w)) \\\n",
    "           .withColumn(\"penalty_max\", max(\"penalty_score\").over(w))\n",
    "\n",
    "    # Normalizar penalty_score a [0, 1]\n",
    "    df = df.withColumn(\n",
    "        \"penalty_score_scaled\",\n",
    "        when(\n",
    "            col(\"penalty_max\") != col(\"penalty_min\"),\n",
    "            (col(\"penalty_score\") - col(\"penalty_min\")) / (col(\"penalty_max\") - col(\"penalty_min\"))\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    # Ajustar performance_score\n",
    "    df = df.withColumn(\n",
    "        \"adjusted_score\",\n",
    "        col(\"performance_score\") - col(\"penalty_score_scaled\")\n",
    "    )\n",
    "\n",
    "    # Limpiar columnas auxiliares si no las necesitas\n",
    "    df = df.drop(\"penalty_min\", \"penalty_max\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df2 = ajustar_penalty_score(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e77a0e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------+--------------+--------------------+-------------------+------------------+\n",
      "|           Player|               Pos|         Squad|   Competition|      adjusted_score|  performance_score|     penalty_score|\n",
      "+-----------------+------------------+--------------+--------------+--------------------+-------------------+------------------+\n",
      "|  bruno fernandes|Attacking Midfield|manchester utd|premier-league| 0.25164164559369295| 0.4600860900381374|0.9380000000000001|\n",
      "|      jakub moder|  Central Midfield|      brighton|premier-league|  0.3638061210290161| 0.3638061210290161|               0.0|\n",
      "|      daley blind|       Centre-Back|        girona|        laliga|  0.2961812019322885| 0.3261812019322885|             0.405|\n",
      "|    tammy abraham|    Centre-Forward|          roma|       serie-a| -0.7395062725298683|0.26049372747013166|              18.0|\n",
      "| manuel locatelli|Defensive Midfield|      juventus|       serie-a| 0.34723869069005425| 0.4654346471441756|             0.532|\n",
      "|  aitor fernandez|        Goalkeeper|       osasuna|        laliga| 0.46554337037197335|0.46554337037197335|               0.0|\n",
      "|  nicola zalewski|     Left Midfield|         inter|       serie-a|-0.36458044261152156| 0.5832918978140101|             1.349|\n",
      "|         raphinha|       Left Winger|     barcelona|        laliga|  0.2387562251567314|0.30598747299634427|             0.778|\n",
      "| antonee robinson|         Left-Back|        fulham|premier-league| 0.13737511292206087| 0.3807084462553942|             0.657|\n",
      "|davide zappacosta|    Right Midfield|      atalanta|       serie-a| 0.30867559359448754| 0.5267487825414628|             0.704|\n",
      "|    michael olise|      Right Winger| bayern munich|    bundesliga| 0.30154074401924447| 0.4847074106859111|             1.099|\n",
      "|     andrei ratiu|        Right-Back|rayo vallecano|        laliga|  0.1690536128830486|0.36127583510527084|             1.038|\n",
      "|       joao felix|    Second Striker|       chelsea|premier-league| 0.20074549165444627|0.41185660276555736|0.9499999999999998|\n",
      "+-----------------+------------------+--------------+--------------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "def obtener_mejores_jugadores_por_posicion(df, temporada=\"2024-2025\"):\n",
    "    # Filtrar temporada y asegurarse de que performance_score no sea nulo\n",
    "    df_filtrado = df.filter((col(\"Season\") == temporada) & (col(\"adjusted_score\").isNotNull()))\n",
    "\n",
    "    # Crear ventana por Evaluated_Position y ordenar por performance_score descendente\n",
    "    window_pos = Window.partitionBy(\"Evaluated_Position\").orderBy(col(\"performance_score\").desc())\n",
    "\n",
    "    # Asignar ranking y quedarte con el top 1 por posición\n",
    "    df_top = df_filtrado.withColumn(\"rank\", row_number().over(window_pos)) \\\n",
    "                        .filter(col(\"rank\") == 2) \\\n",
    "                        .drop(\"rank\")\n",
    "\n",
    "    return df_top\n",
    "\n",
    "obtener_mejores_jugadores_por_posicion(df2).select(\"Player\", \"Pos\", \"Squad\", \"Competition\", \"adjusted_score\", \"performance_score\", \"penalty_score\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
